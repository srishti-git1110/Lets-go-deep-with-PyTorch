{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FFNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM/UY6B33g+lKZyp+2khd2k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srishti-git1110/Simple_NN_in_PyTorch/blob/main/FFNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is just to get a hands on demo of pytorch for ones who know what neural networks are & their working in theory, and want to implement their theoretical knowledge in pytorch.\n",
        "You can run this nb on a gpu as well as cpu.\n",
        "\n",
        "Here we work with feed forward neural networks"
      ],
      "metadata": {
        "id": "rbtBL5tu6DnG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-ZOBVgEThS7"
      },
      "source": [
        "import torch.nn as nn                       #ffnn, cnn, rnn etc\n",
        "import torch.optim as optim                 #optimisers like adam, sgd etc\n",
        "import torch.nn.functional as f             #activ funcs\n",
        "from torch.utils.data import DataLoader     #minibatch train test etc\n",
        "import torchvision.datasets as datasets     #std datasets\n",
        "import torchvision.transforms as transforms #transformations to perform on dataset\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NN(nn.Module):\n",
        "  def __init__(self, input_size, num_classes):\n",
        "    super(NN, self).__init__()\n",
        "\n",
        "    #input layer - hidden layer - output layer; This is our network structure\n",
        "    self.fc1 = nn.Linear(in_features=input_size, out_features=50)\n",
        "    self.fc2 = nn.Linear(in_features=50, out_features=num_classes)\n",
        "\n",
        "  def forward(self, x): ,
        "    \"""\n",
        "    forward pass of the network\n",
        "    x: batch of data, should have shape (batch_size, num_features)\n",
        "    returns the output tensor with shape (batch_size, num_classes)\n",
        "    \"""\n",
        "    #64*768\n",
        "    x = f.relu(self.fc1(x)) \n",
        "    #64*50\n",
        "    x = self.fc2(x)\n",
        "    return x #64*10\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "input_size = 784\n",
        "num_classes = 10\n",
        "learning_rate = .001\n",
        "batch_size = 64\n",
        "epochs = 1\n",
        "\n",
        "#load data\n",
        "train_set = datasets.MNIST(root='dataset/', train=True, transform=transforms.ToTensor(), download=True)\n",
        "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
        "test_set = datasets.MNIST(root='dataset/', train=False, transform=transforms.ToTensor(), download=True)\n",
        "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "#initialize\n",
        "model = NN(input_size, num_classes).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#train\n",
        "for epoch in range(epochs):\n",
        "  for batch_num, (data, targets) in enumerate(train_loader):\n",
        "    data = data.to(device=device)\n",
        "    targets = targets.to(device=device)\n",
        "    data = data.reshape(data.shape[0], -1) #original was (64, 1, 28, 28) we want (64, 768)\n",
        "    \n",
        "    #calls the forward, returns 64*10\n",
        "    scores = model(data) \n",
        "    loss = criterion(scores, targets)\n",
        "    \n",
        "    #backward\n",
        "    optimizer.zero_grad() #clears old gradients from the last step (otherwise youâ€™d just accumulate the gradients from all loss.backward() calls).\n",
        "    loss.backward() #computes the derivative of the loss w.r.t. the parameters (or anything requiring gradients) using backpropagation.\n",
        "    optimizer.step() #causes the optimizer to take a step based on the gradients of the parameters\n",
        "\n",
        "def check_accuracy(loader, model):\n",
        "  if loader.dataset.train:\n",
        "    print(\"checking accuracy on train data\")\n",
        "  else:\n",
        "    print(\"checking accuracy on test data\")\n",
        "\n",
        "  num_correct = 0\n",
        "  num_samples = 0\n",
        "  model.eval() #some training only operations turned off like dropout etc\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x, y in loader: #loops over batches in a loader\n",
        "      x = x.to(device=device)\n",
        "      y = y.to(device=device)\n",
        "      x = x.reshape(x.shape[0], -1)\n",
        "\n",
        "      scores = model(x) #64*10\n",
        "      _, predictions = scores.max(1) #probs, indices\n",
        "      num_correct += (predictions == y).sum()\n",
        "      num_samples += predictions.size(0)\n",
        "    print(f'Accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n",
        "  model.train() #again puts it in train mode just in case \n",
        "  \n",
        "\n",
        "check_accuracy(train_loader, model)\n",
        "check_accuracy(test_loader, model)"
      ],
      "metadata": {
        "id": "dbrevt7aI4tQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
