{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dataset-and-DataPipe-colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNSl6zBxht5CTJT5adYdRNb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srishti-git1110/Lets-go-deep-with-PyTorch/blob/main/Dataset%20and%20DataPipes%20blog/Dataset_and_DataPipe_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting the dataset from kaggle.**\n",
        "\n",
        "Here's the link for you to check it out - https://www.kaggle.com/datasets/lefterislymp/neuralsntua-image-captioning"
      ],
      "metadata": {
        "id": "bgG-0CAJqAY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d lefterislymp/neuralsntua-image-captioning\n",
        "!unzip /content/neuralsntua-image-captioning.zip"
      ],
      "metadata": {
        "id": "3kyKKCWgM2YW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch makes deep learning easier and highly accessible and so, we \"depend\" a lot on it. \n",
        "</br> So, some dependencies -"
      ],
      "metadata": {
        "id": "_7Cs1euTruNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "!pip install transformers\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "H0wUHs4WO89M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Custom** **Dataset** **class**"
      ],
      "metadata": {
        "id": "Wuf19g7QqHsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KaggleImageCaptioningDataset(Dataset):\n",
        "  def __init__(self, train_captions, root_dir, transform=None, bert_model='distilbert-base-uncased', max_len=512):\n",
        "    self.df = pd.read_csv(train_captions, header=None, sep='|')\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(bert_model)\n",
        "    self.max_len = max_len\n",
        "\n",
        "    self.images = self.df.iloc[:,0]\n",
        "    self.captions = self.df.iloc[:,2]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    caption = self.captions[idx]\n",
        "    image_id = self.images[idx]\n",
        "    path_to_image = os.path.join(self.root_dir, image_id)\n",
        "    image = Image.open(path_to_image).convert('RGB')\n",
        "    \n",
        "    if self.transform is not None:\n",
        "      image = self.transform(image)\n",
        "\n",
        "    tokenized_caption = self.tokenizer(caption, \n",
        "                                      padding='max_length',  # Pad to max_length\n",
        "                                      truncation=True,  # Truncate to max_length\n",
        "                                      max_length=self.max_len,  \n",
        "                                      return_tensors='pt')['input_ids']\n",
        "    \n",
        "    return image, tokenized_caption"
      ],
      "metadata": {
        "id": "m1ImL7NdO9K6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's load the data with the mighty DataLoader"
      ],
      "metadata": {
        "id": "c4gOqFKcqN6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = '/content/flickr30k-images-ecemod/image_dir'\n",
        "train_captions = '/content/train_captions.csv'\n",
        "bert_model = 'distilbert-base-uncased'\n",
        "transform = transforms.Compose([transforms.Resize(256),\n",
        "                                transforms.CenterCrop(224),\n",
        "                                transforms.PILToTensor()])\n",
        "train_dataset = KaggleImageCaptioningDataset(train_captions=train_captions,\n",
        "                                       root_dir=root_dir,\n",
        "                                       transform=transform,\n",
        "                                       bert_model=bert_model)\n",
        "train_loader = DataLoader(train_dataset, \n",
        "                          batch_size=64, \n",
        "                          num_workers=2, \n",
        "                          shuffle=True)"
      ],
      "metadata": {
        "id": "pGmsFna8jrJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Hoping everything went right...*"
      ],
      "metadata": {
        "id": "sDfMei_NqURq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_num, (image, caption) in enumerate(train_loader):\n",
        "  if batch_num > 3:\n",
        "    break\n",
        "  print(f'batch number {batch_num} has {image.shape[0]} images and correspondingly {caption.shape[0]} tokenized captions')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3t_uIYFlqIg",
        "outputId": "ae763854-ad39-4560-f6c6-79c6901dd788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch number 0 has 64 images and correspondingly 64 tokenized captions\n",
            "batch number 1 has 64 images and correspondingly 64 tokenized captions\n",
            "batch number 2 has 64 images and correspondingly 64 tokenized captions\n",
            "batch number 3 has 64 images and correspondingly 64 tokenized captions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IT DID !!!\n",
        "\n",
        "Now, let's look at the new DataPipes.\n",
        "\n"
      ],
      "metadata": {
        "id": "6O3tNleHqlCA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataPipes"
      ],
      "metadata": {
        "id": "SPNxkrE53dSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata\n",
        "import torchdata.datapipes as dp\n",
        "from torch.utils.data.backward_compatibility import worker_init_fn\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4iWJioAsIXj",
        "outputId": "2f81ab68-12d3-4c07-8fa4-b25f7986fb21"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.7/dist-packages (0.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.23.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.25.11)\n",
            "Requirement already satisfied: torch==1.12.0 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.12.0)\n",
            "Requirement already satisfied: portalocker>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.5.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.12.0->torchdata) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_csv = '/content/train_captions.csv'\n",
        "train_dp = dp.iter.FileOpener([training_csv])\n",
        "train_dp = train_dp.parse_csv(delimiter='|')\n",
        "train_dp = train_dp.shuffle(buffer_size=2000)\n",
        "train_dp = train_dp.sharding_filter()"
      ],
      "metadata": {
        "id": "BAnW_LAXsIUh"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 512\n",
        "root_dir = '/content/flickr30k-images-ecemod/image_dir'\n",
        "\n",
        "def apply_image_transforms(image):\n",
        "  \n",
        "  transform = transforms.Compose([transforms.Resize(256),\n",
        "                                transforms.CenterCrop(224),\n",
        "                                transforms.PILToTensor()])\n",
        "  return transform(image)\n",
        "\n",
        "def open_image_from_imagepath(row):\n",
        "  image_id, _, caption = row\n",
        "  path_to_image = os.path.join(root_dir, image_id)\n",
        "  image = Image.open(path_to_image).convert('RGB')\n",
        "  image = apply_image_transforms(image)\n",
        "  tokenized_caption = tokenizer(caption, \n",
        "                                padding='max_length',  # Pad to max_length\n",
        "                                truncation=True,  # Truncate to max_length\n",
        "                                max_length=max_len,  \n",
        "                                return_tensors='pt')['input_ids']\n",
        "  return {'image':image, 'caption':tokenized_caption}\n",
        "\n",
        "  \n",
        "train_dp = train_dp.map(open_image_from_imagepath)\n",
        "train_loader = DataLoader(dataset=train_dp, shuffle=True, batch_size=32, num_workers=2, worker_init_fn=worker_init_fn)"
      ],
      "metadata": {
        "id": "edMEM5YesHtj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1\n",
        "bert_model = 'distilbert-base-uncased'    # use any model of your choice\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_model)\n",
        "for epoch in range(num_epochs):\n",
        "  for batch_num, batch_dict in enumerate(train_loader):\n",
        "            if batch_num > 2:\n",
        "                break\n",
        "            \n",
        "            images, captions = batch_dict['image'], batch_dict['caption']\n",
        "            print(f'Batch {batch_num} has {images.shape[0]} images and correspondingly {captions.shape[0]} captions')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eJwMn_L4L0_",
        "outputId": "84f54af6-7736-42d4-f5a3-f48528200251"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/backward_compatibility.py:4: UserWarning: Usage of backward_compatibility.worker_init_fn is deprecated as DataLoader automatically applies sharding in every worker\n",
            "  warnings.warn(\"Usage of backward_compatibility.worker_init_fn is deprecated\"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/backward_compatibility.py:4: UserWarning: Usage of backward_compatibility.worker_init_fn is deprecated as DataLoader automatically applies sharding in every worker\n",
            "  warnings.warn(\"Usage of backward_compatibility.worker_init_fn is deprecated\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 has 32 images and correspondingly 32 captions\n",
            "Batch 1 has 32 images and correspondingly 32 captions\n",
            "Batch 2 has 32 images and correspondingly 32 captions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# That's how the amazing 🤗 tokenizers work!"
      ],
      "metadata": {
        "id": "gJusuG4Cq5TY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "bert_model = 'distilbert-base-uncased'    # use any model of your choice\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_model)\n",
        "tokenizer('hi how are you')"
      ],
      "metadata": {
        "id": "UuLdvsBkO9NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = 'distilbert-base-uncased'    # use any model of your choice\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_model)\n"
      ],
      "metadata": {
        "id": "09ValHR14qNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Thanks for going through my notebook. I hope to see you in a new PyTorch blog of mine!** 👋"
      ],
      "metadata": {
        "id": "zNrOI_iqrDQQ"
      }
    }
  ]
}