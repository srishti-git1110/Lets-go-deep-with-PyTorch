{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dataset-and-DataLoader-blog.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMV1Mt9Fl8UZeTGVpqFfUjL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srishti-git1110/Lets-go-deep-with-PyTorch/blob/main/Dataset_and_DataLoader_blog_DataPipes_added.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3v8bvSPBj8wU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b32d896a-f03f-4d03-8a38-3694bbe07e47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json\n"
          ]
        }
      ],
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting the dataset from kaggle.\n",
        "\n",
        "Here's the link for you to check it out - https://www.kaggle.com/datasets/lefterislymp/neuralsntua-image-captioning"
      ],
      "metadata": {
        "id": "bgG-0CAJqAY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d lefterislymp/neuralsntua-image-captioning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmMvJYKLMjNC",
        "outputId": "ce879284-65ab-4cf9-e8e2-7951aa9936d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neuralsntua-image-captioning.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/neuralsntua-image-captioning.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kyKKCWgM2YW",
        "outputId": "d1af1c0e-5d9d-4a93-cf6e-db233903e16e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/neuralsntua-image-captioning.zip\n",
            "replace flickr30k-images-ecemod/image_dir/_1000070808.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "y\n",
            "n\n",
            "n\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch makes deep learning easier and highly accessible and so, we \"depend\" a lot on it. \n",
        "</br> So, some dependencies -"
      ],
      "metadata": {
        "id": "_7Cs1euTruNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "!pip install transformers\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "H0wUHs4WO89M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "239cda9d-95b5-480c-ec67-c71ad03d3f4c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Custom** **Dataset** **class**"
      ],
      "metadata": {
        "id": "Wuf19g7QqHsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KaggleImageCaptioningDataset(Dataset):\n",
        "  def __init__(self, train_captions, root_dir, transform=None, bert_model='distilbert-base-uncased', max_len=512):\n",
        "    self.df = pd.read_csv(train_captions, header=None, sep='|')\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(bert_model)\n",
        "    self.max_len = max_len\n",
        "\n",
        "    self.images = self.df.iloc[:,0]\n",
        "    self.captions = self.df.iloc[:,2]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    caption = self.captions[idx]\n",
        "    image_id = self.images[idx]\n",
        "    path_to_image = os.path.join(self.root_dir, image_id)\n",
        "    image = Image.open(path_to_image).convert('RGB')\n",
        "    \n",
        "    if self.transform is not None:\n",
        "      image = self.transform(image)\n",
        "\n",
        "    tokenized_caption = self.tokenizer(caption, \n",
        "                                      padding='max_length',  # Pad to max_length\n",
        "                                      truncation=True,  # Truncate to max_length\n",
        "                                      max_length=self.max_len,  \n",
        "                                      return_tensors='pt')['input_ids']\n",
        "    \n",
        "    return image, tokenized_caption"
      ],
      "metadata": {
        "id": "m1ImL7NdO9K6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's load the data with the mighty DataLoader"
      ],
      "metadata": {
        "id": "c4gOqFKcqN6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AUoD1NwvvnKJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = '/content/flickr30k-images-ecemod/image_dir'\n",
        "train_captions = '/content/train_captions.csv'\n",
        "bert_model = 'distilbert-base-uncased'\n",
        "transform = transforms.Compose([transforms.Resize(256),\n",
        "                                transforms.CenterCrop(224),\n",
        "                                transforms.PILToTensor()])\n",
        "train_dataset = KaggleImageCaptioningDataset(train_captions=train_captions,\n",
        "                                       root_dir=root_dir,\n",
        "                                       transform=transform,\n",
        "                                       bert_model=bert_model)\n",
        "train_loader = DataLoader(train_dataset, \n",
        "                          batch_size=64, \n",
        "                          num_workers=2, \n",
        "                          shuffle=True)"
      ],
      "metadata": {
        "id": "pGmsFna8jrJy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hoping everything went right...**"
      ],
      "metadata": {
        "id": "sDfMei_NqURq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_num, (image, caption) in enumerate(train_loader):\n",
        "  if batch_num > 3:\n",
        "    break\n",
        "  print(f'batch number {batch_num} has {image.shape[0]} images and correspondingly {caption.shape[0]} tokenized captions')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3t_uIYFlqIg",
        "outputId": "ae763854-ad39-4560-f6c6-79c6901dd788"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch number 0 has 64 images and correspondingly 64 tokenized captions\n",
            "batch number 1 has 64 images and correspondingly 64 tokenized captions\n",
            "batch number 2 has 64 images and correspondingly 64 tokenized captions\n",
            "batch number 3 has 64 images and correspondingly 64 tokenized captions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IT DID !!!**\n",
        "\n"
      ],
      "metadata": {
        "id": "6O3tNleHqlCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata\n",
        "import torchdata.datapipes as dp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4iWJioAsIXj",
        "outputId": "c3700894-2c9d-4ea7-fafe-2722b03cd964"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.7/dist-packages (0.4.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.25.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.23.0)\n",
            "Requirement already satisfied: torch==1.12.0 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.12.0)\n",
            "Requirement already satisfied: portalocker>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.5.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.12.0->torchdata) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2022.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_csv = '/content/train_captions.csv'\n",
        "train_dp = dp.iter.FileOpener([training_csv])\n",
        "train_dp = train_dp.parse_csv(delimiter='|')"
      ],
      "metadata": {
        "id": "BAnW_LAXsIUh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_length = (pd.read_csv(training_csv, header=None, sep='|')).shape[0]"
      ],
      "metadata": {
        "id": "aTNGrqhOx1b_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dp = train_dp.shuffle(buffer_size=train_length)\n",
        "train_dp = train_dp.sharding_filter()"
      ],
      "metadata": {
        "id": "PnVFaCSssIOk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_image_transforms(image):\n",
        "  \n",
        "  transform = transforms.Compose([transforms.Resize(256),\n",
        "                                transforms.CenterCrop(224),\n",
        "                                transforms.PILToTensor()])\n",
        "  return transform(image)"
      ],
      "metadata": {
        "id": "htKQsdUm2FdQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 512\n",
        "root_dir = '/content/flickr30k-images-ecemod/image_dir'\n",
        "\n",
        "def open_image_from_imagepath(row):\n",
        "  image_id, _, caption = row\n",
        "  path_to_image = os.path.join(root_dir, image_id)\n",
        "  image = Image.open(path_to_image).convert('RGB')\n",
        "  image = apply_image_transforms(image)\n",
        "  tokenized_caption = tokenizer(caption, \n",
        "                                padding='max_length',  # Pad to max_length\n",
        "                                truncation=True,  # Truncate to max_length\n",
        "                                max_length=max_len,  \n",
        "                                return_tensors='pt')['input_ids']\n",
        "  return image, tokenized_caption\n",
        "train_dp = train_dp.map(open_image_from_imagepath)\n",
        "train_dp = train_dp.batch(batch_size=32, drop_last=True)"
      ],
      "metadata": {
        "id": "edMEM5YesHtj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.backward_compatibility import worker_init_fn\n",
        "from torch.utils.data import DataLoader\n",
        "train_loader = DataLoader(dataset=train_dp, shuffle=True, num_workers=2, worker_init_fn=worker_init_fn)"
      ],
      "metadata": {
        "id": "ByAB5cUa3ZQX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#list(DataLoader(dataset=train_dp, num_workers=0))"
      ],
      "metadata": {
        "id": "brKzs6T04-Wo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1\n",
        "bert_model = 'distilbert-base-uncased'    # use any model of your choice\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_model)\n",
        "for epoch in range(num_epochs):\n",
        "  for batch_num, k in enumerate(train_loader):\n",
        "    if batch_num >= 2:\n",
        "      break\n",
        "    print(f'Batch no. {batch_num} has {len(k)} examples')\n",
        "    #print(\" | Batch size:\", k.shape[0], end=\"\")\n",
        "    #print(\" | x shape:\", x.shape, end=\"\")\n",
        "    #print(\" | y shape:\", y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eJwMn_L4L0_",
        "outputId": "4b48f41a-3ae0-44bc-c004-ab6120babdfd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/backward_compatibility.py:4: UserWarning: Usage of backward_compatibility.worker_init_fn is deprecated as DataLoader automatically applies sharding in every worker\n",
            "  warnings.warn(\"Usage of backward_compatibility.worker_init_fn is deprecated\"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/backward_compatibility.py:4: UserWarning: Usage of backward_compatibility.worker_init_fn is deprecated as DataLoader automatically applies sharding in every worker\n",
            "  warnings.warn(\"Usage of backward_compatibility.worker_init_fn is deprecated\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch no. 0 has 32 examples\n",
            "Batch no. 1 has 32 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# That's how the amazing 🤗 tokenizers work!"
      ],
      "metadata": {
        "id": "gJusuG4Cq5TY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "bert_model = 'distilbert-base-uncased'    # use any model of your choice\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_model)\n",
        "tokenizer('hi how are you')"
      ],
      "metadata": {
        "id": "UuLdvsBkO9NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = 'distilbert-base-uncased'    # use any model of your choice\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_model)\n"
      ],
      "metadata": {
        "id": "09ValHR14qNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Thanks for going through my notebook. I hope to see you in a new PyTorch blog of mine!** 👋"
      ],
      "metadata": {
        "id": "zNrOI_iqrDQQ"
      }
    }
  ]
}